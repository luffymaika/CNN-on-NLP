{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "import codecs\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loaddata(path, load_size=100, class_num=5, model=\"train\"):\n",
    "        a = 0\n",
    "        text_list=[]\n",
    "        label_list = []\n",
    "        if model ==\"train\":\n",
    "            for line in open(path,'rb'):\n",
    "                if a == 0:\n",
    "                    a +=1\n",
    "                    continue\n",
    "                elif a<load_size+1 :\n",
    "                    obj = json.loads(line)\n",
    "                    text_list.append(obj[\"text\"])\n",
    "                    label_list.append(obj[\"stars\"])\n",
    "                    a +=1\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            for line in open(path,'rb'):\n",
    "                if a < load_size:\n",
    "                    a +=1\n",
    "                    continue\n",
    "                elif a < load_size*2 :\n",
    "                    obj = json.loads(line)\n",
    "                    text_list.append(obj[\"text\"])\n",
    "                    label_list.append(obj[\"stars\"])\n",
    "                    a +=1\n",
    "                else:\n",
    "                    break\n",
    "        ## make the label to be \"one-hot\" arrary\n",
    "        label = np.zeros([load_size, class_num])\n",
    "        for i,x in enumerate(label_list):\n",
    "            label[i][x-1]  = 1\n",
    "        label = np.array(label)\n",
    "        return text_list,label\n",
    "text_list,label_list = loaddata(\"E:\\\\NLP\\\\Yelp-test\\\\review_10001\", 100, 5, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_punctuation = string.punctuation.replace('.','').replace('?',' ?').replace(' ','').replace('!','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['need inexpensive place stay night two may consider place longer stay id recommend somewhere better amenities  prosgreat location youre right train station central location get old town new town right sight seeing tours  food bars shopping within walking distance  location location location clean good maid serviceconstiny rooms uncomfortable bed absolutely amenities phone room wardrobe given lot attitude husband sharing room quite strange charged  pounds double occupancy sure matters felt like money grab  handled kind odd manner    book hotel get bed desk bathroom  isnt awful know youre getting ']\n"
     ]
    }
   ],
   "source": [
    "def normalize(text_list,stop_words, stop_punctuation):\n",
    "    text_list = [text.lower() for text in text_list]\n",
    "    \n",
    "    text_list = [text.replace('\\n','') for text in text_list]\n",
    "#         text_list = [text.replace('.',\" .\").replace('?',' ?') for text in text_list]\n",
    "    text_list = [''.join(x for x in text if x not in stop_punctuation) for text in text_list]\n",
    "    \n",
    "    text_list = [''.join(x for x in text if x not in \"0123456789\") for text in text_list]\n",
    "#         text2word = [text.split() for text in text_list]\n",
    "        ## text2word:size [text, words]\n",
    "    text2word = [re.split(r'[! ?.]',text) for text in text_list]\n",
    "        # text2word = [[sentence.split() for sentence in text] for text in text2sentence]\n",
    "        # text2word = [sentence.split() for text in text2sentence for sentence in text ]\n",
    "#         text_list = [' '.join(c for c in text if c not in stop_words)for text in text2word]\n",
    "    text_list = [[\" \".join(word for word in text if word not in stop_words)] for text in text2word]\n",
    "    return text_list\n",
    "text_list_normal= normalize(text_list, stop_words, stop_punctuation)\n",
    "print(text_list_normal[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['need', 'inexpensive', 'place', 'stay', 'night', 'two', 'may', 'consider', 'place', 'longer', 'stay', 'id', 'recommend', 'somewhere', 'better', 'amenities', 'prosgreat', 'location', 'youre', 'right', 'train', 'station', 'central', 'location', 'get', 'old', 'town', 'new', 'town', 'right', 'sight', 'seeing', 'tours', 'food', 'bars', 'shopping', 'within', 'walking', 'distance', 'location', 'location', 'location', 'clean', 'good', 'maid', 'serviceconstiny', 'rooms', 'uncomfortable', 'bed', 'absolutely', 'amenities', 'phone', 'room', 'wardrobe', 'given', 'lot', 'attitude', 'husband', 'sharing', 'room', 'quite', 'strange', 'charged', 'pounds', 'double', 'occupancy', 'sure', 'matters', 'felt', 'like', 'money', 'grab', 'handled', 'kind', 'odd', 'manner', 'book', 'hotel', 'get', 'bed', 'desk', 'bathroom', 'isnt', 'awful', 'know', 'youre', 'getting']\n",
      "['need inexpensive place stay night two may consider place longer stay id recommend somewhere better amenities  prosgreat location youre right train station central location get old town new town right sight seeing tours  food bars shopping within walking distance  location location location clean good maid serviceconstiny rooms uncomfortable bed absolutely amenities phone room wardrobe given lot attitude husband sharing room quite strange charged  pounds double occupancy sure matters felt like money grab  handled kind odd manner    book hotel get bed desk bathroom  isnt awful know youre getting ']\n"
     ]
    }
   ],
   "source": [
    "def make_dictionary( text_list_normal , vocabulary_size):\n",
    "        ###    text2word:shape----->[all_text,words]\n",
    "    text2word = [sentence.split() for text in text_list_normal for sentence in text]\n",
    "    print(text2word[0])\n",
    "    print(text_list_normal[0])\n",
    "    wordlist = [word for text in text2word for word in text]\n",
    "#     print(wordlist)\n",
    "        ## 返回前vocabulary_size个常见的词以及频数的tuple（word, num），且按顺序排列\n",
    "    word_diction = collections.Counter(wordlist).most_common(vocabulary_size)\n",
    "    wordlist = [word[0] for word in word_diction]\n",
    "    word_diction = {word:index for index,word in enumerate(wordlist)}\n",
    "    return word_diction,wordlist, text2word\n",
    "word_diction, wordlist ,text_list_normal=make_dictionary(text_list_normal, vocabulary_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91, 467, 4, 181, 182, 55, 117, 772, 4, 773, 181, 240, 147, 468, 43, 318, 774, 11, 183, 56, 184, 118, 469, 11, 12, 148, 149, 92, 149, 56, 775, 776, 777, 0, 778, 779, 470, 471, 472, 11, 11, 11, 33, 1, 780, 781, 150, 782, 185, 473, 318, 319, 34, 783, 474, 151, 475, 241, 320, 34, 79, 784, 785, 786, 242, 787, 243, 788, 321, 6, 244, 322, 789, 186, 476, 790, 323, 48, 12, 185, 791, 152, 245, 477, 80, 183, 478]\n"
     ]
    }
   ],
   "source": [
    "def text2num(wordlist,word_diction, text_list_normal):\n",
    "    textnum = [[word_diction[word] for word in text if word in wordlist] for text in text_list_normal]\n",
    "        ### delete the sentence whice is:[]\n",
    "    for text in textnum:\n",
    "        # for sentence in text:\n",
    "        if text==[]:\n",
    "            textnum.remove([])\n",
    "    return textnum\n",
    "textnum =text2num(wordlist,word_diction,text_list_normal)\n",
    "print(textnum[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "def text_num2normal(textnum):\n",
    "    ## 求出导入数据中文档最大的长度。\n",
    "    lenth = []\n",
    "    for text in textnum:\n",
    "        lenth.append(len(text))\n",
    "    lenth_max = max(lenth)\n",
    "    text_num_normal = []\n",
    "    for text in textnum:\n",
    "        if len(text) < lenth_max:\n",
    "            zero = [0]*(lenth_max-len(text))\n",
    "            text = text+zero\n",
    "        text_num_normal.append(text)\n",
    "    text_num_normal = np.array(text_num_normal)\n",
    "    return text_num_normal\n",
    "text_num_normal = text_num2normal(textnum)\n",
    "print(type(text_num_normal))\n",
    "print(type(text_num_normal[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "append() takes no keyword arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-b9e1f4185e76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: append() takes no keyword arguments"
     ]
    }
   ],
   "source": [
    "a = [[1,2],[2,3,4]]\n",
    "b = [[1,2],[3]]\n",
    "a.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [2, 3, 4], [[1, 2], [3]]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
